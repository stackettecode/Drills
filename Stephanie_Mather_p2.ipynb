{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stephanie_Mather_p2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephmather/Drills/blob/master/Stephanie_Mather_p2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4WrYlX4Mi1j",
        "colab_type": "text"
      },
      "source": [
        "# Project 2: Topic Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knx6wpetMi1k",
        "colab_type": "text"
      },
      "source": [
        "In this project, you'll work with text data from newsgroup postings on a variety of topics. You'll train classifiers to distinguish between the topics based on the text of the posts. Whereas with digit classification, the input is relatively dense: a 28x28 matrix of pixels, many of which are non-zero, here we'll represent each document with a \"bag-of-words\" model. As you'll see, this makes the feature representation quite sparse -- only a few words of the total vocabulary are active in any given document. The bag-of-words assumption here is that the label depends only on the words; their order is not important.\n",
        "\n",
        "The SK-learn documentation on feature extraction will prove useful:\n",
        "http://scikit-learn.org/stable/modules/feature_extraction.html\n",
        "\n",
        "Each problem can be addressed succinctly with the included packages -- please don't add any more. Grading will be based on writing clean, commented code, along with a few short answers.\n",
        "\n",
        "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but please prepare your own write-up and write your own code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNIMozL_Mi1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This tells matplotlib not to try opening a new window for each plot.\n",
        "%matplotlib inline\n",
        "\n",
        "# General libraries.\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# SK-learn libraries for learning.\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# from sklearn.grid_search import GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV # replaced above with correct package\n",
        "\n",
        "# SK-learn libraries for evaluation.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# SK-learn library for importing the newsgroup data.\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# SK-learn libraries for feature extraction from text.\n",
        "from sklearn.feature_extraction.text import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN9RZ1Q8Mi1n",
        "colab_type": "text"
      },
      "source": [
        "Load the data, stripping out metadata so that we learn classifiers that only use textual features. By default, newsgroups data is split into train and test sets. We further split the test so we have a dev set. Note that we specify 4 categories to use for this project. If you remove the categories argument from the fetch function, you'll get all 20 categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3dJ1N2JMi1o",
        "colab_type": "code",
        "outputId": "cceff635-410e-4b0a-e82f-e25443b22365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',\n",
        "                                      remove=('headers', 'footers', 'quotes'),\n",
        "                                      categories=categories)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',\n",
        "                                     remove=('headers', 'footers', 'quotes'),\n",
        "                                     categories=categories)\n",
        "\n",
        "num_test = len(newsgroups_test.target)\n",
        "test_data, test_labels = newsgroups_test.data[round(num_test/2):], newsgroups_test.target[round(num_test/2):]\n",
        "dev_data, dev_labels = newsgroups_test.data[:round(num_test/2)], newsgroups_test.target[:round(num_test/2)]\n",
        "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
        "\n",
        "print ('training label shape:', train_labels.shape)\n",
        "print ('test label shape:', test_labels.shape)\n",
        "print ('dev label shape:', dev_labels.shape)\n",
        "print ('labels names:', newsgroups_train.target_names)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training label shape: (2034,)\n",
            "test label shape: (677,)\n",
            "dev label shape: (676,)\n",
            "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqBeCLHPMi1s",
        "colab_type": "text"
      },
      "source": [
        "(1) For each of the first 5 training examples, print the text of the message along with the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQuj_ukYMi1t",
        "colab_type": "code",
        "outputId": "915240fc-39e0-4f79-d55d-a2de086059cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def P1(num_examples=5):\n",
        "### STUDENT START ###\n",
        "  for i in range(num_examples):\n",
        "    print ('Example: %d\\n Label: %s  \\n Text: \\n%s \\n\\n' %(i+1, newsgroups_train.target_names[train_labels[i]], train_data[i]))\n",
        "\n",
        "  \n",
        "### STUDENT END ###\n",
        "P1()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example: 1\n",
            " Label: comp.graphics  \n",
            " Text: \n",
            "Hi,\n",
            "\n",
            "I've noticed that if you only save a model (with all your mapping planes\n",
            "positioned carefully) to a .3DS file that when you reload it after restarting\n",
            "3DS, they are given a default position and orientation.  But if you save\n",
            "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
            "know why this information is not stored in the .3DS file?  Nothing is\n",
            "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
            "I'd like to be able to read the texture rule information, does anyone have \n",
            "the format for the .PRJ file?\n",
            "\n",
            "Is the .CEL file format available from somewhere?\n",
            "\n",
            "Rych \n",
            "\n",
            "\n",
            "Example: 2\n",
            " Label: talk.religion.misc  \n",
            " Text: \n",
            "\n",
            "\n",
            "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
            "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
            "folks with him, children and all, to satisfy his delusional mania. Jim\n",
            "Jones, circa 1993.\n",
            "\n",
            "\n",
            "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
            "for centuries. \n",
            "\n",
            "\n",
            "Example: 3\n",
            " Label: sci.space  \n",
            " Text: \n",
            "\n",
            " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
            "\n",
            "MB>                                                             So the\n",
            "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
            "\n",
            "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
            "\n",
            "Couldn't we just say periapsis or apoapsis?\n",
            "\n",
            "  \n",
            "\n",
            "\n",
            "Example: 4\n",
            " Label: alt.atheism  \n",
            " Text: \n",
            "I have a request for those who would like to see Charley Wingate\n",
            "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
            "appear to be quite a few of you.)  \n",
            "\n",
            "It is clear that Mr. Wingate intends to continue to post tangential or\n",
            "unrelated articles while ingoring the Challenges themselves.  Between\n",
            "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
            "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
            "\n",
            "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
            "will just go away, and he is doing his level best to change the\n",
            "subject.  Given that this seems a rather common net.theist tactic, I\n",
            "would like to suggest that we impress upon him our desire for answers,\n",
            "in the following manner:\n",
            "\n",
            "1. Ignore any future articles by Mr. Wingate that do not address the\n",
            "Challenges, until he answers them or explictly announces that he\n",
            "refuses to do so.\n",
            "\n",
            "--or--\n",
            "\n",
            "2. If you must respond to one of his articles, include within it\n",
            "something similar to the following:\n",
            "\n",
            "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
            "\n",
            "Really, I'm not looking to humiliate anyone here, I just want some\n",
            "honest answers.  You wouldn't think that honesty would be too much to\n",
            "ask from a devout Christian, would you?  \n",
            "\n",
            "Nevermind, that was a rhetorical question. \n",
            "\n",
            "\n",
            "Example: 5\n",
            " Label: sci.space  \n",
            " Text: \n",
            "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
            "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
            "\n",
            "Does anyone know more about this?  How much, to attend????\n",
            "\n",
            "Anyone want to go? \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpieIoDRMi1v",
        "colab_type": "text"
      },
      "source": [
        "(2) Use CountVectorizer to turn the raw training text into feature vectors. You should use the fit_transform function, which makes 2 passes through the data: first it computes the vocabulary (\"fit\"), second it converts the raw text into feature vectors using the vocabulary (\"transform\").\n",
        "\n",
        "The vectorizer has a lot of options. To get familiar with some of them, write code to answer these questions:\n",
        "\n",
        "a. The output of the transform (also of fit_transform) is a sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html. What is the size of the vocabulary? What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero? Hint: use \"nnz\" and \"shape\" attributes.\n",
        "\n",
        "b. What are the 0th and last feature strings (in alphabetical order)? Hint: use the vectorizer's get_feature_names function.\n",
        "\n",
        "c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. Confirm the training vectors are appropriately shaped. Now what's the average number of non-zero features per example?\n",
        "\n",
        "d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram character features. What size vocabulary does this yield?\n",
        "\n",
        "e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. What size vocabulary does this yield?\n",
        "\n",
        "f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? Hint: build a vocabulary for both train and dev and look at the size of the difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeG85plxMi1v",
        "colab_type": "code",
        "outputId": "744816f2-79b0-48aa-bebd-34fee051f46e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "def P2(train_data, dev_data):\n",
        "### STUDENT START ###\n",
        "\n",
        " ## Part a\n",
        "   # create the transform\n",
        "  vectorizer = CountVectorizer()\n",
        "  \n",
        "  # tokenize and build vocab then encode document\n",
        "  vector = vectorizer.fit_transform(train_data)\n",
        "  \n",
        "  # summarize what was tokenized and the encoded vector\n",
        "  print (\"Part a. What is the size of the vocabulary? \\nWhat is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero?\")   \n",
        "  print ('The size of the vocabulary is', vector.shape[1], \"words.\")  \n",
        "  average_non_zero = [vector[i].nnz for i in range(vector.shape[0])]\n",
        "  print (\"The average number of non-zero features per example is\", round(np.asarray(average_non_zero).mean(),2))\n",
        "  print(\"The fraction of the entries in the matrix that are non zero is\",round(np.asarray(average_non_zero).mean()/vector.shape[1],4))\n",
        "  \n",
        " ## Part b\n",
        "  print(\"\\nPart b. What are the 0th and last feature strings (in alphabetical order)?\")\n",
        "  print(\"The first feature string is '\"+ vectorizer.get_feature_names()[0]+ \"' and the last feature string is '\"+ vectorizer.get_feature_names()[-1]+\"'.\")\n",
        "  \n",
        "  \n",
        "  ## Part c\n",
        "  print('\\nPart c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"].\\nConfirm the training vectors are appropriately shaped. \\nNow what\\'s the average number of non-zero features per example?')\n",
        "  # list of text vocabulary\n",
        "  text = [\"atheism\", \"graphics\", \"space\", \"religion\"]\n",
        "  # tokenize and build vocab\n",
        "  vectorizer.fit(text)\n",
        "  # summarize what was tokenized\n",
        "  print(\"The vocabulary is: \",vectorizer.vocabulary_)\n",
        "  # encode document using vocab\n",
        "  vector = vectorizer.transform(train_data)\n",
        "  \n",
        "  print ('The size of the vocabulary is now', vector.shape[1], \"words. The training vector shape is:\",vector.shape, \", which is as expected for 2034 samples\")  \n",
        "  average_non_zero = [vector[i].nnz for i in range(vector.shape[0])]\n",
        "  print (\"The average number of non-zero features per example for the 4 word vocabulary is\", round(np.asarray(average_non_zero).mean(),2))\n",
        "  print(\"The fraction of the entries in the martix that are non zero is now\",round(np.asarray(average_non_zero).mean()/vector.shape[1],4))\n",
        "  \n",
        "  \n",
        "  ## Part d\n",
        "  print ('\\nPart d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\"\\n to extract bigram and trigram character features. \\nWhat size vocabulary does this yield?')\n",
        "   # create the transform for bigram and trigram features\n",
        "  vectorizer = CountVectorizer(ngram_range=(2, 3), analyzer = 'char')\n",
        "  # tokenize and build vocab then encode document\n",
        "  vector = vectorizer.fit_transform(train_data)\n",
        "  # summarize what was tokenized and the encoded vector\n",
        "  print ('The size of the vocabulary is', vector.shape[1], \"bigrams and trigram characters.\") \n",
        "  average_non_zero = [vector[i].nnz for i in range(vector.shape[0])]\n",
        "  print (\"The average number of non-zero features per example is\", round(np.asarray(average_non_zero).mean(),2))\n",
        "  print(\"The fraction of the entries in the matrix that are non zero are\",round(np.asarray(average_non_zero).mean()/vector.shape[1],4))\n",
        "  print(\"The first feature string is \"+ \"'\" +vectorizer.get_feature_names()[0]+ \"' and the last feature string is '\"+ vectorizer.get_feature_names()[-1]+\"'.\")\n",
        "  \n",
        "  \n",
        "  print ('\\nPart e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. \\nWhat size vocabulary does this yield?')\n",
        "  # create the transform for unigram features, limited to features that appear in more than 10 documents\n",
        "  vectorizer = CountVectorizer(min_df=10)\n",
        "  # tokenize and build vocab then encode document\n",
        "  vector = vectorizer.fit_transform(train_data)\n",
        "  # summarize what was tokenized and the encoded vector\n",
        "  print ('The size of the vocabulary is', vector.shape[1], \"words.\") \n",
        "  average_non_zero = [vector[i].nnz for i in range(vector.shape[0])]\n",
        "  print (\"The average number of non-zero features per example is\", round(np.asarray(average_non_zero).mean(),2))\n",
        "  print(\"The fraction of the entries in the matrix that are non zero is\",round(np.asarray(average_non_zero).mean()/vector.shape[1],4))\n",
        "  print(\"The first feature string is '\"+ vectorizer.get_feature_names()[0]+ \"' and the last feature string is '\"+ vectorizer.get_feature_names()[-1]+\"'.\")\n",
        "\n",
        "  print ('\\nPart f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? \\nHint: build a vocabulary for both train and dev and look at the size of the difference.')\n",
        "  # create the standard transform\n",
        "  vectorizer = CountVectorizer()\n",
        "  # tokenize and build vocab then encode document\n",
        "  vector1 = vectorizer.fit_transform(train_data)\n",
        "  vocab1 = vectorizer.vocabulary_\n",
        "  vector2 = vectorizer.fit_transform(dev_data)\n",
        "  vocab2 = vectorizer.vocabulary_\n",
        "  # get difference between two vocab lists\n",
        "  diff_vocab = set(vocab2) - set(vocab1)\n",
        "  print(\"The fraction of words in the dev_data set that are not in the training set are: \", round(len(diff_vocab)/len(set(vocab2)),3))\n",
        "### STUDENT END ###\n",
        "P2(train_data, dev_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Part a. What is the size of the vocabulary? \n",
            "What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero?\n",
            "The size of the vocabulary is 26879 words.\n",
            "The average number of non-zero features per example is 96.71\n",
            "The fraction of the entries in the matrix that are non zero is 0.0036\n",
            "\n",
            "Part b. What are the 0th and last feature strings (in alphabetical order)?\n",
            "The first feature string is '00' and the last feature string is 'zyxel'.\n",
            "\n",
            "Part c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"].\n",
            "Confirm the training vectors are appropriately shaped. \n",
            "Now what's the average number of non-zero features per example?\n",
            "The vocabulary is:  {'atheism': 0, 'graphics': 1, 'space': 3, 'religion': 2}\n",
            "The size of the vocabulary is now 4 words. The training vector shape is: (2034, 4) , which is as expected for 2034 samples\n",
            "The average number of non-zero features per example for the 4 word vocabulary is 0.27\n",
            "The fraction of the entries in the martix that are non zero is now 0.0671\n",
            "\n",
            "Part d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\"\n",
            " to extract bigram and trigram character features. \n",
            "What size vocabulary does this yield?\n",
            "The size of the vocabulary is 35478 bigrams and trigram characters.\n",
            "The average number of non-zero features per example is 684.19\n",
            "The fraction of the entries in the matrix that are non zero are 0.0193\n",
            "The first feature string is '\bi' and the last feature string is 'þ h'.\n",
            "\n",
            "Part e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. \n",
            "What size vocabulary does this yield?\n",
            "The size of the vocabulary is 3064 words.\n",
            "The average number of non-zero features per example is 72.68\n",
            "The fraction of the entries in the matrix that are non zero is 0.0237\n",
            "The first feature string is '00' and the last feature string is 'zip'.\n",
            "\n",
            "Part f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? \n",
            "Hint: build a vocabulary for both train and dev and look at the size of the difference.\n",
            "The fraction of words in the dev_data set that are not in the training set are:  0.248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g8xnR_kMi1x",
        "colab_type": "text"
      },
      "source": [
        " (3) Use the default CountVectorizer options and report the f1 score (use metrics.f1_score) for a k nearest neighbors classifier; find the optimal value for k. Also fit a Multinomial Naive Bayes model and find the optimal value for alpha. Finally, fit a logistic regression model and find the optimal value for the regularization strength C using l2 regularization. A few questions:\n",
        "\n",
        "a. Why doesn't nearest neighbors work well for this problem?\n",
        "\n",
        "b. Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
        "\n",
        "c. Logistic regression estimates a weight vector for each class, which you can access with the coef\\_ attribute. Output the sum of the squared weight values for each class for each setting of the C parameter. Briefly explain the relationship between the sum and the value of C."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgnoCu6oMBjN",
        "colab_type": "code",
        "outputId": "f6d0dba2-21c9-4805-be83-1116175c9ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "## Nearest Neighboours model\n",
        "\n",
        "def P3a_knn():\n",
        " \n",
        "  # Process data\n",
        "  pro_train_data, pro_dev_data, vectorizer = preprocessor(train_data, dev_data)\n",
        "  \n",
        "  #knn model\n",
        "  k_values = [1, 3, 5, 7, 9, 21, 31, 51, 81, 101, 121, 151, 201, 251, 300]\n",
        "  knn_model(k_values, pro_train_data, pro_dev_data, train_labels, dev_labels)\n",
        "  \n",
        "  #best k\n",
        "  model = KNeighborsClassifier(n_neighbors=121)\n",
        "  my_score = get_score(model, pro_train_data, pro_dev_data, train_labels, dev_labels)\n",
        "  print(\"\\n\\nThe best K-Nearest Neighbors is with k: %s and the f1 score is %s\" %(121, round(my_score,3)))\n",
        "\n",
        "  return pro_train_data, pro_dev_data\n",
        "  \n",
        "def preprocessor(train_data, dev_data, **kwargs):\n",
        "  vectorizer = CountVectorizer(**kwargs)\n",
        "  pro_train_data = vectorizer.fit_transform(train_data)\n",
        "  pro_dev_data = vectorizer.transform(dev_data)\n",
        "  return pro_train_data, pro_dev_data, vectorizer\n",
        "\n",
        "def knn_model(k_values, train_data, dev_data, train_labels, dev_labels):\n",
        "  \"\"\"fit K-Nearest-Neighbors model with different number of k using the mini training set and return score\"\"\"\n",
        "  # compare score with differing values of k_values\n",
        "  for k_value in k_values:\n",
        "      model = KNeighborsClassifier(n_neighbors=k_value)\n",
        "      my_score = get_score(model, train_data, dev_data, train_labels, dev_labels)\n",
        "      print(\"K-Nearest Neighbors with k: %s the f1 score is %s\" %(k_value, round(my_score,3)))\n",
        "\n",
        "def get_score(model, train_X, val_X, train_y, val_y):\n",
        "  \"\"\"Function to return single accuracy score for a model\"\"\"  \n",
        "  model.fit(train_X, train_y)\n",
        "  pred_y = model.predict(val_X)\n",
        "  accuracy = metrics.f1_score(val_y, pred_y, average = 'micro') \n",
        "  return(accuracy)\n",
        "\n",
        "\n",
        "  ### STUDENT END ###\n",
        "pro_train_data, pro_dev_data = P3a_knn()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K-Nearest Neighbors with k: 1 the f1 score is 0.383\n",
            "K-Nearest Neighbors with k: 3 the f1 score is 0.414\n",
            "K-Nearest Neighbors with k: 5 the f1 score is 0.423\n",
            "K-Nearest Neighbors with k: 7 the f1 score is 0.444\n",
            "K-Nearest Neighbors with k: 9 the f1 score is 0.43\n",
            "K-Nearest Neighbors with k: 21 the f1 score is 0.423\n",
            "K-Nearest Neighbors with k: 31 the f1 score is 0.447\n",
            "K-Nearest Neighbors with k: 51 the f1 score is 0.419\n",
            "K-Nearest Neighbors with k: 81 the f1 score is 0.451\n",
            "K-Nearest Neighbors with k: 101 the f1 score is 0.459\n",
            "K-Nearest Neighbors with k: 121 the f1 score is 0.467\n",
            "K-Nearest Neighbors with k: 151 the f1 score is 0.442\n",
            "K-Nearest Neighbors with k: 201 the f1 score is 0.454\n",
            "K-Nearest Neighbors with k: 251 the f1 score is 0.428\n",
            "K-Nearest Neighbors with k: 300 the f1 score is 0.413\n",
            "\n",
            "\n",
            "The best K-Nearest Neighbors is with k: 121 and the f1 score is 0.467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3-vltHlMzG4",
        "colab_type": "text"
      },
      "source": [
        "a. Why doesn't nearest neighbors work well for this problem?\n",
        "\n",
        "T*he sparse nature of the matrix produced by the 'bag of words' approach means that there are a large number of features per sample. There are also many features which are common to all samples such as stop words (eg. 'the', 'and', or 'a'). This makes it hard for the kNN model to effectively group the text samples together in a high dimensional space.*\n",
        "\n",
        "*We get a very rough clumping of the data as witnessed by an increase over the base accuracy (25% for 4 labels randomly allocated), but little effect of changing the k parameter.  If the kNN model was more suitable for the problem we would expect to see more of a trend between k and the f1 score.*\n",
        "\n",
        "*Also apparent is the 'curse of dimensionality' as the kNN model is slow in this instance.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGiRyvRrN9g5",
        "colab_type": "code",
        "outputId": "0bb7a976-1993-4f8b-f285-e33b53dee5eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def P3b_mNB():\n",
        "  \"\"\"Function to train and optimise Multinomial Naive Bayes classifier\"\"\"\n",
        "  \n",
        "  # Gridsearch parameters\n",
        "  alphas = {'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
        "  MultinomialNB_model(alphas, pro_train_data, pro_dev_data, train_labels, dev_labels)\n",
        "  \n",
        "def MultinomialNB_model(alphas, train_data, dev_data, train_labels, dev_labels):\n",
        "  \"\"\" Function to Grid search Multinomial Naive Bayes classifier for best alpha\"\"\"\n",
        "  gridsearch = GridSearchCV(MultinomialNB(), alphas, cv=3)\n",
        "  gridsearch.fit(train_data, train_labels,)\n",
        "  grid_predict = gridsearch.predict(dev_data)\n",
        "  \n",
        "  #Report accuracy\n",
        "  my_score = get_score(gridsearch, train_data, dev_data, train_labels, dev_labels)\n",
        "  print (\"\\n\\nMultinomial NB classifier is optimized with an alpha of %f, giving a f1 score of %.3f\\n\\n\" %(gridsearch.best_params_['alpha'], my_score))\n",
        "\n",
        "  return gridsearch\n",
        "\n",
        "P3b_mNB()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Multinomial NB classifier is optimized with an alpha of 0.010000, giving a f1 score of 0.780\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9ih1v2AMi1y",
        "colab_type": "code",
        "outputId": "c6014176-5316-4412-cf91-4aaae68dcbfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def P3c_log():\n",
        "  \"\"\"Function to report f1_score and sum-squared of the coef of matrix for LogisticRegression model\"\"\"\n",
        "  \n",
        "  #Report accuracy of best value for C\n",
        "  C=0.25\n",
        "  logreg=LogisticRegression(C=C, solver = 'liblinear', multi_class = 'ovr', max_iter = 1000)\n",
        "  logreg.fit(pro_train_data,train_labels)\n",
        "  my_score = get_score(logreg, pro_train_data, pro_dev_data, train_labels, dev_labels)\n",
        "  print (\"\\n\\nLogistic Regression classifier is optimized with a regularization strength  of %f, giving a f1 score of %f\\n\\n\" %(C, my_score))\n",
        "\n",
        "\n",
        "  ## Logistic Regression\n",
        "  C_values = {\"C\":[0.001, 0.0025, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 10, 25, 50, 100, 250, 500, 1000]}\n",
        "  \n",
        "  for C in C_values[\"C\"]:\n",
        "    LogisticRegression_model(C, pro_train_data, pro_dev_data, train_labels, dev_labels)\n",
        "\n",
        "  \n",
        "  \n",
        "def LogisticRegression_model(C, train_data, dev_data, train_labels, dev_labels):\n",
        "  \"\"\" Function apply Logistic Regression classifier for best C, the regularisation strength. Report out f1_score and sum-squared weight matrix \"\"\"\n",
        "  logreg=LogisticRegression(C=C, solver = 'liblinear', multi_class = 'ovr', max_iter = 1000)\n",
        "  logreg.fit(train_data,train_labels)\n",
        "\n",
        "\n",
        "  #report accuracy and coef matrix\n",
        "  my_score = get_score(logreg, train_data, dev_data, train_labels, dev_labels)\n",
        "  print (\"\\nLogistic Regression classifier with a regularization strength of %f has f1 score of %.3f and a sum-squared weight matrix of \\n\" %(C, my_score))\n",
        "  print(np.sum(np.square(logreg.coef_),1))\n",
        "  return logreg\n",
        "  \n",
        "### STUDENT END ###\n",
        "P3c_log()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Logistic Regression classifier is optimized with a regularization strength  of 0.250000, giving a f1 score of 0.714497\n",
            "\n",
            "\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 0.001000 has f1 score of 0.638 and a sum-squared weight matrix of \n",
            "\n",
            "[0.16509345 0.20095275 0.18067094 0.18724278]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 0.002500 has f1 score of 0.649 and a sum-squared weight matrix of \n",
            "\n",
            "[0.50408039 0.63011287 0.57425932 0.51458396]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 0.005000 has f1 score of 0.664 and a sum-squared weight matrix of \n",
            "\n",
            "[1.14370284 1.39815203 1.31636967 1.07816519]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 0.010000 has f1 score of 0.679 and a sum-squared weight matrix of \n",
            "\n",
            "[2.54153098 2.93970937 2.86246912 2.25002927]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 0.025000 has f1 score of 0.678 and a sum-squared weight matrix of \n",
            "\n",
            "[6.93995704 7.26868972 7.44620469 5.88517805]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 0.050000 has f1 score of 0.697 and a sum-squared weight matrix of \n",
            "\n",
            "[14.07353836 13.66874191 14.60465921 11.8606793 ]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 0.100000 has f1 score of 0.704 and a sum-squared weight matrix of \n",
            "\n",
            "[27.13206789 24.6631043  27.45846367 23.02489271]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 0.250000 has f1 score of 0.714 and a sum-squared weight matrix of \n",
            "\n",
            "[59.82418156 50.62286959 58.94364139 51.47421142]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 0.500000 has f1 score of 0.714 and a sum-squared weight matrix of \n",
            "\n",
            "[102.60267689  83.1206713   99.06034276  88.99963978]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 1.000000 has f1 score of 0.700 and a sum-squared weight matrix of \n",
            "\n",
            "[166.97926241 130.93321704 158.0154707  145.77551398]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 2.500000 has f1 score of 0.695 and a sum-squared weight matrix of \n",
            "\n",
            "[293.00628122 224.27884655 271.96564824 258.32500229]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 10.000000 has f1 score of 0.691 and a sum-squared weight matrix of \n",
            "\n",
            "[585.2640372  448.02967683 539.62536162 530.67716982]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 25.000000 has f1 score of 0.688 and a sum-squared weight matrix of \n",
            "\n",
            "[857.73091721 661.5403974  791.53816007 791.64774191]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 50.000000 has f1 score of 0.685 and a sum-squared weight matrix of \n",
            "\n",
            "[1111.71839462  864.01808842 1024.29303989 1033.97474635]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 100.000000 has f1 score of 0.686 and a sum-squared weight matrix of \n",
            "\n",
            "[1409.42220078 1082.29598509 1284.39399484 1324.12614831]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 250.000000 has f1 score of 0.685 and a sum-squared weight matrix of \n",
            "\n",
            "[1850.95214824 1457.12340813 1735.57766159 1762.29383183]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 500.000000 has f1 score of 0.683 and a sum-squared weight matrix of \n",
            "\n",
            "[2268.10485505 1714.81503885 2111.40726515 2067.59206404]\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 1000.000000 has f1 score of 0.685 and a sum-squared weight matrix of \n",
            "\n",
            "[2713.59252354 1927.77733901 2167.47795272 2451.45314405]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtRa18WyOi5g",
        "colab_type": "text"
      },
      "source": [
        "b. Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
        "\n",
        "*The text classification problem is a problem that is high dimensional: it has a large number of features (vocabulary words), with a sparse number present in each sample. In addition, the number of samples is << number of features. The simplifying effect of the independence assumption means the Naive Bayes is better able to handle a large number of features per sample and the required number of samples for effective training is lower than for logistic regression. NB is also good at ignoring irrelevant features, such as the stop words which appear in most samples ('the, and' or 'a'), logistic regression is less able to do this. As such, logistic regression may work better if we could increase the size of the training set or employ some feature engineering.*\n",
        "\n",
        "\n",
        "c. Logistic regression estimates a weight vector for each class, which you can access with the coef\\_ attribute. Output the sum of the squared weight values for each class for each setting of the C parameter. Briefly explain the relationship between the sum and the value of C.\n",
        "\n",
        "*Constraining a model to make it simpler and reduce the risk of overfitting is called regularisation. C is the inverse of the amount of regularisation of the features in the Linear Regression model, i.e. a small C means the model has a high regularisation strength and will try to fit the majority of the features at the expense of accuracy across the entire data set and a large C means the model has little to no regularisation and will try to fit to each feature equally as the model tries to correctly classify every data point.*\n",
        "\n",
        "*This is reflected in the weighting given to each feature. As C in increased the absolute weighting of each feature is increased as the model tried to 'fit' to all the data. By using a smaller C, the model gets better at generalising to new data, but too much generalisation and useful information is thrown away and accruacy drops again. For this set of training and development data, a C=0.25 gives the best accuracy.*\n",
        "\n",
        "*In summary, C is the parameter to control the strength of regularization*\n",
        "*  lower C => log_reg adjusts to the majority of data points.\n",
        "*  higher C => correct classification of each data point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUxjopZDMi11",
        "colab_type": "text"
      },
      "source": [
        "(4) Train a logistic regression model. Find the 5 features with the largest weights for each label -- 20 features in total. Create a table with 20 rows and 4 columns that shows the weight for each of these features for each of the labels. Create the table again with bigram features. Any surprising features in this table?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA3_SpjJMi12",
        "colab_type": "code",
        "outputId": "88a0924f-bae4-4635-e2f3-33fe3f947db1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def P4():\n",
        "### STUDENT START ###\n",
        "\n",
        "\n",
        "## Unigram\n",
        "# Process data\n",
        "  pro_train_data, pro_dev_data, vectorizer = preprocessor(train_data, dev_data)\n",
        "\n",
        "## Logistic Regression\n",
        "  C=0.1\n",
        "  lg = LogisticRegression_model(C, pro_train_data, pro_dev_data, train_labels, dev_labels)\n",
        "  print_table(lg, vectorizer)\n",
        "  \n",
        "## Bigram\n",
        "# Process data\n",
        "  countvectarg = {'ngram_range':(2,2), 'analyzer' : 'word'}\n",
        "  pro_train_data, pro_dev_data, vectorizer = preprocessor(train_data, dev_data, **countvectarg)\n",
        "\n",
        "## Logistic Regression\n",
        "  C=0.1\n",
        "  lg = LogisticRegression_model(C, pro_train_data, pro_dev_data, train_labels, dev_labels)\n",
        "  print_table(lg, vectorizer)  \n",
        "  \n",
        "  \n",
        "def print_table(lg, vectorizer):\n",
        "  indices = np.argpartition(lg.coef_, -5, axis=1)[:, -5:]\n",
        " \n",
        "  wordlist = []\n",
        "  for i in range(4):\n",
        "  #label names\n",
        "    print ('\\n\\nLabel:', newsgroups_train.target_names[i])\n",
        "  #feature names\n",
        "    print ('Top 5 features:')\n",
        "    for j in range(5):\n",
        "      word = vectorizer.get_feature_names()[indices[i][j]]\n",
        "      print(word, end = \", \" )\n",
        "      wordlist.append(word)\n",
        "\n",
        "    print ('\\nFeatures weight:')\n",
        "    for j in range(5):\n",
        "      print(round(lg.coef_[i][indices[i][j]],3), end = \", \" )\n",
        "        \n",
        "  print()\n",
        "  \n",
        "  # Build weight DataFrame for easy printing\n",
        "  data = [lg.coef_[i][indices.flatten()].tolist() for i in range(4)]  \n",
        "  data_print = pd.DataFrame(np.asarray(data).T, columns =  newsgroups_train.target_names, index = wordlist)\n",
        "  \n",
        "\n",
        "  print (\"\\n\\nThe 5 features with the largest weights for each label -- 20 features in total are:\\n\")\n",
        "  print (data_print)\n",
        "  \n",
        "### STUDENT END ###\n",
        "P4()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Logistic Regression classifier with a regularization strength of 0.100000 has f1 score of 0.704 and a sum-squared weight matrix of \n",
            "\n",
            "[27.13206789 24.6631043  27.45846367 23.02489271]\n",
            "\n",
            "\n",
            "Label: alt.atheism\n",
            "Top 5 features:\n",
            "islam, atheists, bobby, atheism, religion, \n",
            "Features weight:\n",
            "0.426, 0.461, 0.478, 0.496, 0.494, \n",
            "\n",
            "Label: comp.graphics\n",
            "Top 5 features:\n",
            "3d, computer, image, file, graphics, \n",
            "Features weight:\n",
            "0.547, 0.559, 0.642, 0.641, 1.008, \n",
            "\n",
            "Label: sci.space\n",
            "Top 5 features:\n",
            "moon, launch, nasa, space, orbit, \n",
            "Features weight:\n",
            "0.403, 0.479, 0.541, 1.259, 0.597, \n",
            "\n",
            "Label: talk.religion.misc\n",
            "Top 5 features:\n",
            "fbi, christians, blood, order, christian, \n",
            "Features weight:\n",
            "0.422, 0.499, 0.434, 0.429, 0.548, \n",
            "\n",
            "\n",
            "The 5 features with the largest weights for each label -- 20 features in total are:\n",
            "\n",
            "            alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
            "islam          0.426305      -0.084871  -0.165053           -0.164858\n",
            "atheists       0.461443      -0.079453  -0.158392           -0.295313\n",
            "bobby          0.478148      -0.120418  -0.167848           -0.227832\n",
            "atheism        0.495521      -0.207137  -0.199912           -0.267835\n",
            "religion       0.493967      -0.298856  -0.393226            0.003913\n",
            "3d            -0.181992       0.547049  -0.311634           -0.181452\n",
            "computer      -0.039776       0.558953  -0.329045           -0.228713\n",
            "image         -0.263612       0.642034  -0.367627           -0.216103\n",
            "file          -0.177350       0.641247  -0.421577           -0.288282\n",
            "graphics      -0.411164       1.007701  -0.651127           -0.372214\n",
            "moon          -0.202805      -0.291498   0.403123           -0.056436\n",
            "launch        -0.210984      -0.244652   0.478962           -0.168969\n",
            "nasa          -0.273472      -0.261910   0.540874           -0.253108\n",
            "space         -0.655246      -0.714305   1.258779           -0.590205\n",
            "orbit         -0.217587      -0.334110   0.597351           -0.248965\n",
            "fbi           -0.127869      -0.111739  -0.220494            0.422100\n",
            "christians    -0.330871      -0.162767  -0.203044            0.499420\n",
            "blood         -0.208267      -0.063147  -0.097593            0.433855\n",
            "order         -0.354744      -0.037615  -0.081877            0.429110\n",
            "christian     -0.262426      -0.190443  -0.194524            0.547522\n",
            "\n",
            "Logistic Regression classifier with a regularization strength of 0.100000 has f1 score of 0.617 and a sum-squared weight matrix of \n",
            "\n",
            "[32.69484005 35.09050825 35.93078596 28.4056332 ]\n",
            "\n",
            "\n",
            "Label: alt.atheism\n",
            "Top 5 features:\n",
            "are you, claim that, in this, is not, cheers kent, \n",
            "Features weight:\n",
            "0.277, 0.285, 0.298, 0.296, 0.315, \n",
            "\n",
            "Label: comp.graphics\n",
            "Top 5 features:\n",
            "comp graphics, is there, out there, in advance, looking for, \n",
            "Features weight:\n",
            "0.383, 0.416, 0.463, 0.526, 0.671, \n",
            "\n",
            "Label: sci.space\n",
            "Top 5 features:\n",
            "it was, the space, the moon, sci space, and such, \n",
            "Features weight:\n",
            "0.328, 0.525, 0.563, 0.368, 0.37, \n",
            "\n",
            "Label: talk.religion.misc\n",
            "Top 5 features:\n",
            "jesus christ, the word, the fbi, with you, cheers kent, \n",
            "Features weight:\n",
            "0.246, 0.255, 0.32, 0.274, 0.33, \n",
            "\n",
            "\n",
            "The 5 features with the largest weights for each label -- 20 features in total are:\n",
            "\n",
            "               alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
            "are you           0.277471      -0.144993  -0.078735           -0.147018\n",
            "claim that        0.285116      -0.102313  -0.140065           -0.055676\n",
            "in this           0.297924      -0.026339  -0.295854           -0.065220\n",
            "is not            0.296119      -0.146012  -0.287450            0.030932\n",
            "cheers kent       0.314807      -0.366991  -0.355327            0.329548\n",
            "comp graphics    -0.152040       0.382660  -0.203534           -0.127415\n",
            "is there         -0.176670       0.415799  -0.274608           -0.122940\n",
            "out there        -0.175384       0.462993  -0.274904           -0.163800\n",
            "in advance       -0.282799       0.525638  -0.258337           -0.245845\n",
            "looking for      -0.383644       0.671229  -0.292240           -0.327732\n",
            "it was           -0.105407      -0.200366   0.328214           -0.167434\n",
            "the space        -0.172213      -0.289088   0.524647           -0.171627\n",
            "the moon         -0.231810      -0.309019   0.562894           -0.157118\n",
            "sci space        -0.147057      -0.202693   0.367722           -0.124180\n",
            "and such         -0.123206      -0.202601   0.370203           -0.135291\n",
            "jesus christ     -0.107959      -0.087030  -0.114937            0.245855\n",
            "the word          0.017182      -0.138737  -0.156157            0.255371\n",
            "the fbi          -0.071426      -0.112654  -0.162918            0.320273\n",
            "with you         -0.116106      -0.009126  -0.182095            0.273992\n",
            "cheers kent       0.314807      -0.366991  -0.355327            0.329548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPSBSm6-Mi16",
        "colab_type": "text"
      },
      "source": [
        "ANSWER:\n",
        "\n",
        "*The unigram features are hevaily weighted towards the top 5 features (most coef are >0.5) and the choice of words are mostly as expected. However some stand out:*\n",
        "* 'bobby' for atheisim is unexpected and may just suggest there is a prolific member of the forum called bobby.\n",
        "* 'the words 'graphics' and 'space' have coeffs >1 thus are very stong indicators that the text is classified as comp.graphics and sci.space. This may cause misclassification of other texts that include these words.\n",
        "* 'religion' is a strong predictor for atheism which is unsuprising given the crossover between the two subjects but may cause reduced accuracy for the classifier when trying to distinguish between religuos and atheist texts.\n",
        "\n",
        "*The bigram features reflect the weaker performance of this model. Generally the coef <0.5 for the top 5 features. The makeup of the bigrams do not often reflect the topic and also includes many common stopwords such as the phrases 'it was' 'and such'. Only the space and religion catagories have bigrams that are recognisibly associated with the topic. It is concluded that bigrams are not a useful feature of this data set.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjrjrEN-Mi17",
        "colab_type": "text"
      },
      "source": [
        "(5) Try to improve the logistic regression classifier by passing a custom preprocessor to CountVectorizer. The preprocessing function runs on the raw text, before it is split into words by the tokenizer. Your preprocessor should try to normalize the input in various ways to improve generalization. For example, try lowercasing everything, replacing sequences of numbers with a single token, removing various other non-letter characters, and shortening long words. If you're not already familiar with regular expressions for manipulating strings, see https://docs.python.org/2/library/re.html, and re.sub() in particular. With your new preprocessor, how much did you reduce the size of the dictionary?\n",
        "\n",
        "For reference, I was able to improve dev F1 by 2 points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd83BR65Mi18",
        "colab_type": "code",
        "outputId": "89f436f0-91ef-4e92-a1da-4078c3645ec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def empty_preprocessor(s):\n",
        "   return s\n",
        "\n",
        "\n",
        "def better_preprocessor(s):\n",
        "  \"\"\"A custom pre-processor for CountVectoriser\"\"\"\n",
        "  \n",
        "  s = s.lower() # lowercase all letters\n",
        "  s = re.sub(\"[^\\w\\s]\", \" \", s) # remove all special characters and replace with whitespace\n",
        "  words = s.split()\n",
        "  words = map(remove_num, words) #replace all numbers\n",
        "  words = map(remove_suffix, words) #remove common suffixes\n",
        "  words = map(shorten_words, words) #shorten words to length 6\n",
        "  s = \" \".join(words)\n",
        "  return s\n",
        "\n",
        "def remove_num(word):\n",
        "  \"\"\"Replace all numbers with a single 0\"\"\"\n",
        "  return re.sub(r\"[0-9]+\", \"0\", word)\n",
        "\n",
        "def shorten_words(word):\n",
        "  \"Shortens words to length specified. If word is of shorter length, the word is returned\"\n",
        "  shorten_amount = 6\n",
        "  if len(word) > shorten_amount:\n",
        "    return word[:shorten_amount]\n",
        "  else:\n",
        "    return word \n",
        "  \n",
        "def remove_suffix(word):\n",
        "  \"\"\"Removes pre-defined suffix from word\"\"\"\n",
        "  \n",
        "  suffixes = ['tion', '\\'s', 's', 'ly', 'ing', 'ty', 'ment', 'ant', 'ship', 'age', 'ery']\n",
        "  for suffix in suffixes:\n",
        "    word = re.sub(suffix + \"$\", \"\", word)\n",
        "  return word\n",
        "  \n",
        "\n",
        "def P5():\n",
        "## STUDENT START ###\n",
        "  \"\"\"Fit logistic regression model to the training data using the count vectorizer with a custom preprocessor.\n",
        "    The resulting vocabulary length and f1 score is reported.\"\"\"\n",
        "  for C in [0.25]: \n",
        "    logreg_pipe_no_proc = Pipeline([\n",
        "        ('vectors', CountVectorizer(preprocessor = empty_preprocessor)),\n",
        "        ('logistic_reg', LogisticRegression(C=C, penalty = \"l2\", solver = 'liblinear', multi_class = 'ovr', max_iter =1000))])\n",
        "\n",
        "    logreg_pipe_no_proc.fit(train_data, train_labels)\n",
        "\n",
        "    print(\"The length of the vocabulary without preprocessor is\", len(logreg_pipe_no_proc.named_steps[\"vectors\"].vocabulary_))\n",
        "    print(\"f1 score: {0:.3f}\".format(metrics.f1_score(dev_labels, logreg_pipe_no_proc.predict(dev_data),  average = \"micro\")))\n",
        "\n",
        "    logreg_pipe_proc = Pipeline([\n",
        "        ('vectors', CountVectorizer(preprocessor = better_preprocessor)),\n",
        "        ('logistic_reg', LogisticRegression(C=C, penalty = \"l2\", solver = 'liblinear', multi_class = 'ovr', max_iter =1000))])\n",
        "\n",
        "    logreg_pipe_proc.fit(train_data, train_labels)  \n",
        "\n",
        "    print(C, \"The length of the vocabulary with the better processor is\", len(logreg_pipe_proc.named_steps[\"vectors\"].vocabulary_))\n",
        "    print(\"f1 score: {0:.3f}\".format(metrics.f1_score(dev_labels, logreg_pipe_proc.predict(dev_data), average = \"micro\")))\n",
        "\n",
        "  \n",
        "## STUDENT END ###\n",
        "P5()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of the vocabulary without preprocessor is 33291\n",
            "f1 score: 0.710\n",
            "0.25 The length of the vocabulary with the better processor is 16606\n",
            "f1 score: 0.735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh6ibDkbBGQW",
        "colab_type": "text"
      },
      "source": [
        "*I was able to improve the f1 score from 0.710 to 0.735 with the above preprocessor and I reduced the dictionary size by roughly half.  My preprocessor did the following manipulations:*\n",
        "\n",
        "*   lowercase all letters\n",
        "*   remove all special characters and replace with whitespace\n",
        "*   replace all numbers with '0'\n",
        "*   remove common suffixes\n",
        "*   shorten words to length 6 (note this was tuned as a hyperparameter with the development data)\n",
        "\n",
        "**Note:** \n",
        "\n",
        "*The choice of multiclass hyperparameter is dependent on the data set but it was observed that the performance of the pre-processor is very sensitive to C if the multiclass type used is multinomial. In fact, varying C could make the preprocessor either help or harm the model.  The model becomes more stable using the 'ovr' (one-versus-rest), i.e C has a much smaller affect on the accuracy. For this reason a 'ovr' multi-class is used in with this dataset for it's repeatability.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbwWNF3SMi1-",
        "colab_type": "text"
      },
      "source": [
        "(6) The idea of regularization is to avoid learning very large weights (which are likely to fit the training data, but not generalize well) by adding a penalty to the total size of the learned weights. That is, logistic regression seeks the set of weights that minimizes errors in the training data AND has a small size. The default regularization, L2, computes this size as the sum of the squared weights (see P3, above). L1 regularization computes this size as the sum of the absolute values of the weights. The result is that whereas L2 regularization makes all the weights relatively small, L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
        "\n",
        "Train a logistic regression model using a \"l1\" penalty. Output the number of learned weights that are not equal to zero. How does this compare to the number of non-zero weights you get with \"l2\"? Now, reduce the size of the vocabulary by keeping only those features that have at least one non-zero weight and retrain a model using \"l2\".\n",
        "\n",
        "Make a plot showing accuracy of the re-trained model vs. the vocabulary size you get when pruning unused features by adjusting the C parameter.\n",
        "\n",
        "Note: The gradient descent code that trains the logistic regression model sometimes has trouble converging with extreme settings of the C parameter. Relax the convergence criteria by setting tol=.01 (the default is .0001)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVo9cdmbMi1_",
        "colab_type": "code",
        "outputId": "4e56bd93-280a-4aec-b467-7dd7bdfbd068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        }
      },
      "source": [
        "def LogisticRegression_model2(C=0.25, penalty = \"l2\", vocabulary = None):\n",
        "  \"\"\" Function to fit Logistic Regression classifier\"\"\"\n",
        "  \n",
        "  train_data, dev_data, train_labels, dev_labels\n",
        "  logreg_pipe=Pipeline([('vectors', CountVectorizer(vocabulary=vocabulary)), \n",
        "                        ('logistic_reg', \n",
        "                         LogisticRegression(C=C, penalty = penalty, \n",
        "                                            solver = 'liblinear', \n",
        "                                            multi_class = 'ovr', \n",
        "                                            max_iter = 1000, tol = 0.01))])\n",
        "  logreg_pipe.fit(train_data,train_labels)\n",
        "  \n",
        "  return logreg_pipe\n",
        "\n",
        "def zero_weight_percent(coefs):\n",
        "  \"Returns the percentage of coeffient weights in a matrix that are non-zero\"\n",
        "  return 100*(np.count_nonzero(coefs))/coefs.size\n",
        "\n",
        "def P6():\n",
        "  # Keep this random seed here to make comparison easier.\n",
        "  np.random.seed(0)\n",
        "\n",
        "  ### STUDENT START ###\n",
        "  # Train logistic regression model with l2 penalty\n",
        "  logreg_l2 = LogisticRegression_model2()\n",
        "  coefs = logreg_l2.named_steps[\"logistic_reg\"].coef_\n",
        "  vocab_max = logreg_l2.named_steps[\"vectors\"].get_feature_names()\n",
        "\n",
        "  print('L2 penalty non-zero weight %: {:.3f}\\n'.format(zero_weight_percent(coefs)))\n",
        "  \n",
        "  # Train logistic regression model with l1 penalty for various sizes of C\n",
        "  \n",
        "  #initialise empty lists\n",
        "  reduced_vocab_len=[]\n",
        "  f1_list =  []\n",
        "  \n",
        "  # Iterate through increasing regularisation strengths\n",
        "  for C in [0.001, 0.0025, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 10, 25, 50, 100, 250, 500, 1000]: #np.logspace(-3,3,28):\n",
        "    logreg_l1 = LogisticRegression_model2(C=C, penalty = \"l1\")\n",
        "    coefs = logreg_l1.named_steps[\"logistic_reg\"].coef_\n",
        "\n",
        "    print('L1 penalty non-zero weight %: {:.3f} for C: {} \\n'.format(zero_weight_percent(coefs), C))\n",
        "\n",
        "    #get reduced vocab from the L1 model\n",
        "    feature_indices = np.nonzero(coefs)[1]\n",
        "    reduced_vocab = (set(np.take(vocab_max, feature_indices))) \n",
        "    \n",
        "    #fit L2 model with the reduced vocab\n",
        "    logreg_min_l2 = LogisticRegression_model2(vocabulary = reduced_vocab)\n",
        "    dev_predict = logreg_min_l2.predict(dev_data)\n",
        "    f1 = metrics.f1_score(dev_labels, dev_predict, average = \"micro\")\n",
        "\n",
        "    #Create list of f1 and reduced vocab size for L2 models\n",
        "    reduced_vocab_len.append(len(reduced_vocab))\n",
        "    f1_list.append(f1)\n",
        "    \n",
        "  #plot f1 and vocabulary length for the L2 models\n",
        "  plt.scatter(reduced_vocab_len, f1_list)\n",
        "    ### STUDENT END ###\n",
        "P6()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2 penalty non-zero weight %: 100.000\n",
            "\n",
            "L1 penalty non-zero weight %: 0.005 for C: 0.001 \n",
            "\n",
            "L1 penalty non-zero weight %: 0.010 for C: 0.0025 \n",
            "\n",
            "L1 penalty non-zero weight %: 0.020 for C: 0.005 \n",
            "\n",
            "L1 penalty non-zero weight %: 0.033 for C: 0.01 \n",
            "\n",
            "L1 penalty non-zero weight %: 0.089 for C: 0.025 \n",
            "\n",
            "L1 penalty non-zero weight %: 0.193 for C: 0.05 \n",
            "\n",
            "L1 penalty non-zero weight %: 0.336 for C: 0.1 \n",
            "\n",
            "L1 penalty non-zero weight %: 0.699 for C: 0.25 \n",
            "\n",
            "L1 penalty non-zero weight %: 1.155 for C: 0.5 \n",
            "\n",
            "L1 penalty non-zero weight %: 1.696 for C: 1.0 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "L1 penalty non-zero weight %: 3.013 for C: 2.5 \n",
            "\n",
            "L1 penalty non-zero weight %: 4.883 for C: 10 \n",
            "\n",
            "L1 penalty non-zero weight %: 7.998 for C: 25 \n",
            "\n",
            "L1 penalty non-zero weight %: 12.188 for C: 50 \n",
            "\n",
            "L1 penalty non-zero weight %: 17.199 for C: 100 \n",
            "\n",
            "L1 penalty non-zero weight %: 25.101 for C: 250 \n",
            "\n",
            "L1 penalty non-zero weight %: 44.174 for C: 500 \n",
            "\n",
            "L1 penalty non-zero weight %: 58.221 for C: 1000 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Bu9OkIvLkO",
        "colab_type": "text"
      },
      "source": [
        "ANSWER: \n",
        "*The plot above shows that a vocabulary greater that ~1000 words does not add much additional accuracy to the model, and by ~4000 words there is no gain for additional words. Thus you can use a smaller vocabulary to decrease computation time for the L2 model without sacrificing accuracy.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxkspPlKMi2B",
        "colab_type": "text"
      },
      "source": [
        "(7) Use the TfidfVectorizer -- how is this different from the CountVectorizer? Train a logistic regression model with C=100.\n",
        "\n",
        "Make predictions on the dev data and show the top 3 documents where the ratio R is largest, where R is:\n",
        "\n",
        "maximum predicted probability / predicted probability of the correct label\n",
        "\n",
        "What kinds of mistakes is the model making? Suggest a way to address one particular issue that you see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhuUEokjMi2C",
        "colab_type": "code",
        "outputId": "3bea8e24-9bff-4091-df18-814cc5901557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def P7(stop_words = None, C=100):\n",
        "### STUDENT START ###\n",
        "  \"\"\"Fit a logistic regression model using the TfidfVectorizer. Report the worst three predictions from the development set\"\"\"\n",
        "\n",
        "  tf_log_pipe = Pipeline([('vectors', TfidfVectorizer(sublinear_tf = False, \n",
        "                                                      stop_words = stop_words)),\n",
        "                          ('logistic_reg', LogisticRegression(C=C,  \n",
        "                                                              penalty = 'l2', \n",
        "                                                              solver = 'liblinear', \n",
        "                                                              multi_class = 'ovr'))])\n",
        "\n",
        "  tf_log_pipe.fit(train_data, train_labels)\n",
        "  dev_predict = tf_log_pipe.predict(dev_data)\n",
        "\n",
        "  f1 = metrics.f1_score(dev_labels, dev_predict,  average = \"micro\")\n",
        "  print(\"f1 Score: {0:.3f}\\n\".format(f1))\n",
        "  \n",
        "  # Extract probabilities.\n",
        "  probabilities = tf_log_pipe.predict_proba(dev_data)\n",
        "\n",
        "  # Generate a list of ratios from the propability vectors.\n",
        "  r_list = np.asarray(list(map(ratio_r, enumerate(probabilities))))\n",
        "\n",
        "  # Find the top 3.\n",
        "  r_list_index = np.argsort(r_list)[:-4:-1]\n",
        "  for  i in r_list_index:\n",
        "    print(\"R = {}, Correct = {}, Pred = {}\\n\".format(r_list[i],\n",
        "    newsgroups_train.target_names[dev_labels[i]],\n",
        "    newsgroups_train.target_names[dev_predict[i]]))\n",
        "    print(dev_data[i])\n",
        "    print()\n",
        "    \n",
        "  # print the confusion matrix\n",
        "  # Compute confusion matrix\n",
        "  cm = confusion_matrix(dev_labels, dev_predict)\n",
        "  print (\"\\nThe confusion matrix for the model using TfidfVectorizer\")\n",
        "  # Plot non-normalized confusion matrix\n",
        "  plot_confusion_matrix(cm = cm, classes=  newsgroups_train.target_names,\n",
        "                      title='Confusion matrix, News Group Classifier')\n",
        "  \n",
        "  return tf_log_pipe\n",
        "\n",
        "def ratio_r(x):\n",
        "  \"\"\"Calculate the ratio of maximum predicted probability to predicted probability\n",
        "  of the correct label in the dev set.\n",
        "  \"\"\"\n",
        "  n = x[0]\n",
        "  prob = x[1]\n",
        "  20\n",
        "  max_prob = np.max(prob)\n",
        "  correct = prob[dev_labels[n]]\n",
        "  return max_prob / correct\n",
        "\n",
        "# Code for plot_confusion_matrix adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function plots the confusion matrix.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], 'd'),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "## STUDENT END ###\n",
        "P7()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 Score: 0.763\n",
            "\n",
            "R = 929.3571373334202, Correct = talk.religion.misc, Pred = comp.graphics\n",
            "\n",
            "I am pleased to announce that a *revised version* of _The Easy-to-Read Book\n",
            "of Mormon_ (former title: _Mormon's Book_) by Lynn Matthews Anderson is now\n",
            "available through anonymous ftp (see information below). In addition to the\n",
            "change in title, the revised ETR BOM has been shortened by several pages\n",
            "(eliminating many extraneous \"that's\" and \"of's\"), and many (minor) errors\n",
            "have been corrected. This release includes a simplified Joseph Smith Story,\n",
            "testimonies of the three and eight witnesses, and a \"Words-to-Know\"\n",
            "glossary.\n",
            "\n",
            "As with the previous announcement, readers are reminded that this is a\n",
            "not-for-profit endeavor. This is a copyrighted work, but people are welcome\n",
            "to make *verbatim* copies for personal use. People can recuperate the\n",
            "actual costs of printing (paper, copy center charges), but may not charge\n",
            "anything for their time in making copies, or in any way realize a profit\n",
            "from the use of this book. See the permissions notice in the book itself\n",
            "for the precise terms.\n",
            "\n",
            "Negotiations are currently underway with a Mormon publisher vis-a-vis the\n",
            "printing and distribution of bound books. (Sorry, I'm out of the wire-bound\n",
            "\"first editions.\") I will make another announcement about the availability\n",
            "of printed copies once everything has been worked out.\n",
            "\n",
            "FTP information: connect via anonymous ftp to carnot.itc.cmu.edu, then \"cd\n",
            "pub\" (you won't see anything at all until you do).\n",
            "\n",
            "\"The Easy-to-Read Book of Mormon\" is currently available in postscript and\n",
            "RTF (rich text format). (ASCII, LaTeX, and other versions can be made\n",
            "available; contact dba@andrew.cmu.edu for details.) You should be able to\n",
            "print the postscript file on any postscript printer (such as an Apple\n",
            "Laserwriter); let dba know if you have any difficulties. (The postscript in\n",
            "the last release had problems on some printers; this time it should work\n",
            "better.) RTF is a standard document interchange format that can be read in\n",
            "by a number of word processors, including Microsoft Word for both the\n",
            "Macintosh and Windows. If you don't have a postscript printer, you may be\n",
            "able to use the RTF file to print out a copy of the book.\n",
            "\n",
            "-r--r--r--  1 dba                   1984742 Apr 27 13:12 etrbom.ps\n",
            "-r--r--r--  1 dba                   1209071 Apr 27 13:13 etrbom.rtf\n",
            "\n",
            "For more information about how this project came about, please refer to my\n",
            "article in the current issue of _Sunstone_, entitled \"Delighting in\n",
            "Plainness: Issues Surrounding a Simple Modern English Book of Mormon.\"\n",
            "\n",
            "Send all inquiries and comments to:\n",
            "\n",
            "    Lynn Matthews Anderson\n",
            "    5806 Hampton Street\n",
            "    Pittsburgh, PA 15206\n",
            "\n",
            "R = 325.0038462992751, Correct = talk.religion.misc, Pred = comp.graphics\n",
            "\n",
            "Can anyone provide me a ftp site where I can obtain a online version\n",
            "of the Book of Mormon. Please email the internet address if possible.\n",
            "\n",
            "R = 287.3072077917014, Correct = alt.atheism, Pred = talk.religion.misc\n",
            "\n",
            "\n",
            "The 24 children were, of course, killed by a lone gunman in a second story\n",
            "window, who fired eight bullets in the space of two seconds...\n",
            "\n",
            "\n",
            "\n",
            "The confusion matrix for the model using TfidfVectorizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAEYCAYAAAAzhB+DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez5zyHwbMi2E",
        "colab_type": "text"
      },
      "source": [
        "ANSWER: *The CountVectorizer in its original form is an integer count of the number of time a word from the corpus occurs in the sample. The corpus is defined by each unique word in the training set. The TfidVectorizer is a mathematical attempt to filter out the impact of words that are common to a language and thus carry little meaning to the content of a documents (e.g.“the”, “a”, “is” in English). It allows the benefits of a 'stop word list' but without the pitfalls of choosing a custom list that is unsuitable for the dataset. TfidVectorizer does this by providing a floating number score for each word which increases proportionally to count, but is offset by the frequency of the word in the corpus. This is the IDF (inverse document frequency part).This means that the words appear more frequently score lower than using a simple CountVectorizer.*\n",
        "\n",
        "\n",
        "*For this dataset, the TfidVectorizer preprocessor is more accurate than the CountVectorizer, giving an f1 score  of 0.763. Analysing top three mismatches by the TfidVectorizer:*\n",
        "\n",
        "\n",
        "1.  This religion article has been incorrectly assigned the comp.graphics label instead of talk.religion.misc. Although this text is about the Book of Mormon, it refers to several common computer sciecne terms: postscript, ftp, ASCII and LaTex. These terms would be more common in texts about graphics, hence the missclassification.\n",
        "2.   This example is a short text which refers to getting an online copy of the 'Book of Mormon'. It was assigned the comp.graphics label instead of talk.religion.misc. The misclassification is also likely due to the presence of common words related to the internet such as 'online' and 'ftp'.\n",
        "3. The third text is an example of the most frequent error by the model, confusing talk.religion.misc and alt.athesim labels. This is evident in the confusion matrix above. In this case the text was mislabeled talk.religion.misc instead of athesim. This is likely due to the common overlap in the two subjects, in this case the shooting may have been refering to an act of religious extremism iif it was contained in a larger text experpt to give it religious context. However, in this case it refers to a shooting without referal to religion.  **This could be done by removing the vocabulary that has a relatively high weight for both labels from the model.**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NIU727eMi2E",
        "colab_type": "text"
      },
      "source": [
        "(8) EXTRA CREDIT\n",
        "\n",
        "Try implementing one of your ideas based on your error analysis. Use logistic regression as your underlying model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvmOzmddKrBQ",
        "colab_type": "text"
      },
      "source": [
        "I decided to try and reduce the confusion between the talk.religion.misc and alt.athesim labels by removing the vocabulary that has a relatively high weight for both labels from the model. I compared the coef_ for the top 100 words in each catagory and added the word to a stop_words list if the weight of the opposite label was >-0.5. This was to remove words from the vocabulary set that were relatively common to both catagories. The expacted outcome was to reduce the number of confused labels between talk.religion.misc and alt.athesim.\n",
        "\n",
        "\n",
        "As can be seen below,  this approach was successful in increasing the accuracy from 0.763 to 0.769,  a small increase in accuracy. This approach was sensitive to both the number of words chosen to compare (100 was most beneficial) and the cut-off for the lesser weight (-0.5 used).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGyrMIYmMi2F",
        "colab_type": "code",
        "outputId": "c9ffff5b-6873-49ac-88d0-9a7715895c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def P8(C=100, number_words = 100):\n",
        "### STUDENT START ###\n",
        "  \"\"\"Train a logistic regression model using the TfidfVectorizer preprocessor. Finds the top #number_words weighted words in both talk.religion.misc and alt.athesim labels. \n",
        "  Then extracts words which are have relatively high weights for the opposite catagory (greater than -0.5)\n",
        "  and builds a custom combined_stop_word list that can be used in a custom TfidfVectorizer preprocessor\"\"\"\n",
        "\n",
        "  tf_log_pipe = Pipeline([('vectors', TfidfVectorizer(sublinear_tf = False)),\n",
        "                          ('logistic_reg', LogisticRegression(C=C,  \n",
        "                                                              penalty = 'l2', \n",
        "                                                              solver = 'liblinear', \n",
        "                                                              multi_class = 'ovr'))])\n",
        "\n",
        "  tf_log_pipe.fit(train_data, train_labels)\n",
        "  dev_predict = tf_log_pipe.predict(dev_data)\n",
        "\n",
        "  #Get top number of words (number_words) in both talk.religion.misc and alt.athesim labels\n",
        "  indices = np.argpartition(tf_log_pipe.named_steps[\"logistic_reg\"].coef_, -(number_words), axis=1)[:, -(number_words):]\n",
        "\n",
        "  #alt.athesim labels corresponding to religion top 50 where coef>-1.5\n",
        "  athesim_stop_words = [tf_log_pipe.named_steps[\"vectors\"].get_feature_names()[indices[0][j]] for j in  range(50) if -0.5 <= tf_log_pipe.named_steps[\"logistic_reg\"].coef_[0][indices[3][j]]]\n",
        "  print('athesim_stop_words:', athesim_stop_words)\n",
        "   #religion labels corresponding to atheism top 50 where coef>-1.5\n",
        "  religion_stop_words = [tf_log_pipe.named_steps[\"vectors\"].get_feature_names()[indices[3][j]] for j in  range(50) if -0.5 <=tf_log_pipe.named_steps[\"logistic_reg\"].coef_[3][indices[0][j]] ]\n",
        "  print('religion_stop_words:', religion_stop_words)\n",
        "  #combine stop_word lists\n",
        "  combined_stop_words = athesim_stop_words + religion_stop_words\n",
        "  print('combined_stop_words:', combined_stop_words)\n",
        "  \n",
        "  coefs = tf_log_pipe.named_steps[\"logistic_reg\"].coef_\n",
        "  vocab_max = tf_log_pipe.named_steps[\"vectors\"].get_feature_names()\n",
        "  \n",
        "  return combined_stop_words\n",
        "\n",
        "## STUDENT END ###\n",
        "# combined_stop_words = P8(number_words = 50)\n",
        "combined_stop_words = P8(number_words = 100)\n",
        "# combined_stop_words = P8(number_words = 75)\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "athesim_stop_words: ['arguments', 'amen', 'people', 'palestinians', 'worse', 'alternative']\n",
            "religion_stop_words: ['taoism', 'story', 'fake', 'that', 'jose', 'josephus', 'truth', 'compuserve']\n",
            "combined_stop_words: ['arguments', 'amen', 'people', 'palestinians', 'worse', 'alternative', 'taoism', 'story', 'fake', 'that', 'jose', 'josephus', 'truth', 'compuserve']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKfPLoYFEs1q",
        "colab_type": "text"
      },
      "source": [
        "**Next step:** Remove the words from the vocabulary that have been identified as common confusion between religion and athesim and retrain the model from P7.\n",
        "\n",
        "Words:\n",
        "['arguments', 'amen', 'people', 'palestinians', 'worse', 'alternative', 'taoism', 'story', 'fake', 'that', 'jose', 'josephus', 'truth', 'compuserve']\n",
        "\n",
        "*It is evident from the R scores that this increase confusion for the example 1 and 2, but it had a significant reduction on the 3rd example which was the error I was trying to address. The R score for that example dropped out of the top 3 (originally 287), and the next worse example (which was of a similar type) dropped  to 154.*\n",
        "\n",
        "*The confusion matrix also saw a small improvement across all the categories.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3CplX9c-mWY",
        "colab_type": "code",
        "outputId": "abf42e14-a996-452f-a1fb-122a46b625c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "P7(stop_words =  combined_stop_words, C=100)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 Score: 0.769\n",
            "\n",
            "R = 1196.1981526082907, Correct = talk.religion.misc, Pred = comp.graphics\n",
            "\n",
            "I am pleased to announce that a *revised version* of _The Easy-to-Read Book\n",
            "of Mormon_ (former title: _Mormon's Book_) by Lynn Matthews Anderson is now\n",
            "available through anonymous ftp (see information below). In addition to the\n",
            "change in title, the revised ETR BOM has been shortened by several pages\n",
            "(eliminating many extraneous \"that's\" and \"of's\"), and many (minor) errors\n",
            "have been corrected. This release includes a simplified Joseph Smith Story,\n",
            "testimonies of the three and eight witnesses, and a \"Words-to-Know\"\n",
            "glossary.\n",
            "\n",
            "As with the previous announcement, readers are reminded that this is a\n",
            "not-for-profit endeavor. This is a copyrighted work, but people are welcome\n",
            "to make *verbatim* copies for personal use. People can recuperate the\n",
            "actual costs of printing (paper, copy center charges), but may not charge\n",
            "anything for their time in making copies, or in any way realize a profit\n",
            "from the use of this book. See the permissions notice in the book itself\n",
            "for the precise terms.\n",
            "\n",
            "Negotiations are currently underway with a Mormon publisher vis-a-vis the\n",
            "printing and distribution of bound books. (Sorry, I'm out of the wire-bound\n",
            "\"first editions.\") I will make another announcement about the availability\n",
            "of printed copies once everything has been worked out.\n",
            "\n",
            "FTP information: connect via anonymous ftp to carnot.itc.cmu.edu, then \"cd\n",
            "pub\" (you won't see anything at all until you do).\n",
            "\n",
            "\"The Easy-to-Read Book of Mormon\" is currently available in postscript and\n",
            "RTF (rich text format). (ASCII, LaTeX, and other versions can be made\n",
            "available; contact dba@andrew.cmu.edu for details.) You should be able to\n",
            "print the postscript file on any postscript printer (such as an Apple\n",
            "Laserwriter); let dba know if you have any difficulties. (The postscript in\n",
            "the last release had problems on some printers; this time it should work\n",
            "better.) RTF is a standard document interchange format that can be read in\n",
            "by a number of word processors, including Microsoft Word for both the\n",
            "Macintosh and Windows. If you don't have a postscript printer, you may be\n",
            "able to use the RTF file to print out a copy of the book.\n",
            "\n",
            "-r--r--r--  1 dba                   1984742 Apr 27 13:12 etrbom.ps\n",
            "-r--r--r--  1 dba                   1209071 Apr 27 13:13 etrbom.rtf\n",
            "\n",
            "For more information about how this project came about, please refer to my\n",
            "article in the current issue of _Sunstone_, entitled \"Delighting in\n",
            "Plainness: Issues Surrounding a Simple Modern English Book of Mormon.\"\n",
            "\n",
            "Send all inquiries and comments to:\n",
            "\n",
            "    Lynn Matthews Anderson\n",
            "    5806 Hampton Street\n",
            "    Pittsburgh, PA 15206\n",
            "\n",
            "R = 333.226249885181, Correct = talk.religion.misc, Pred = comp.graphics\n",
            "\n",
            "Can anyone provide me a ftp site where I can obtain a online version\n",
            "of the Book of Mormon. Please email the internet address if possible.\n",
            "\n",
            "R = 154.00835918971345, Correct = alt.atheism, Pred = talk.religion.misc\n",
            "\n",
            "With the Southern Baptist Convention convening this June to consider\n",
            "the charges that Freemasonry is incompatible with christianity, I thought\n",
            "the following quotes by Mr. James Holly, the Anti-Masonic Flag Carrier,\n",
            "would amuse you all...\n",
            "\n",
            "     The following passages are exact quotes from \"The Southern \n",
            "Baptist Convention and Freemasonry\" by James L. Holly, M.D., President\n",
            "of Mission and Ministry To Men, Inc., 550 N 10th St., Beaumont, TX \n",
            "77706. \n",
            " \n",
            "     The inside cover of the book states: \"Mission & Ministry to Men, \n",
            "Inc. hereby grants permission for the reproduction of part or all of \n",
            "this booklet with two provisions: one, the material is not changed and\n",
            "two, the source is identified.\" I have followed these provisions. \n",
            "  \n",
            "     \"Freemasonry is one of the allies of the Devil\" Page iv. \n",
            " \n",
            "     \"The issue here is not moderate or conservative, the issue is God\n",
            "and the Devil\" Page vi.\" \n",
            " \n",
            "     \"It is worthwhile to remember that the formulators of public \n",
            "school education in America were Freemasons\" Page 29. \n",
            " \n",
            "     \"Jesus Christ never commanded toleration as a motive for His \n",
            "disciples, and toleration is the antithesis of the Christian message.\"\n",
            "Page 30. \n",
            " \n",
            "     \"The central dynamic of the Freemason drive for world unity \n",
            "through fraternity, liberty and equality is toleration. This is seen \n",
            "in the writings of the 'great' writers of Freemasonry\". Page 31. \n",
            " \n",
            "     \"He [Jesus Christ] established the most sectarian of all possible \n",
            "faiths.\" Page 37. \n",
            " \n",
            "     \"For narrowness and sectarianism, there is no equal to the Lord \n",
            "Jesus Christ\". Page 40. \n",
            " \n",
            "     \"What seems so right in the interest of toleration and its \n",
            "cousins-liberty, equality and fraternity-is actually one of the \n",
            "subtlest lies of the 'father of lies.'\" Page 40. \n",
            " \n",
            "     \"The Southern Baptist Convention has many churches which were \n",
            "founded in the Lodge and which have corner stones dedicated by the \n",
            "Lodge. Each of these churches should hold public ceremonies of \n",
            "repentance and of praying the blood and the Name of the Lord Jesus \n",
            "Christ over the church and renouncing the oaths taken at the \n",
            "dedication of the church and/or building.\" Page 53-54.  \n",
            " \n",
            "\n",
            "\n",
            "The confusion matrix for the model using TfidfVectorizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAEYCAYAAAAzhB+DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naZL/ASkiIXQ8s9VoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMpppiyFfvbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}